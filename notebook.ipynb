{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import plotly.express as px\n",
    "from Levenshtein import distance\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train labels\n",
    "train_labels = pd.read_csv(\"bms-molecular-translation/train_labels.csv\")\n",
    "train_labels['InChI'] = train_labels['InChI'].apply(lambda x: x.replace('InChI=', ''))\n",
    "train_labels = train_labels.set_index(\"image_id\")\n",
    "print(f\"Size training set: {len(train_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing\n",
    "text = ''.join(train_labels['InChI'].values)\n",
    "\n",
    "# Vocab\n",
    "vocab = [' '] + sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Mapping\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Max length\n",
    "max_len = max([len(x) for x in train_labels['InChI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image data, create target and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "random_state=0\n",
    "\n",
    "# Parameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "lr=1e-3\n",
    "name=f'gsk'\n",
    "new_shape=[128, 128]\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = get_model(max_len, vocab)\n",
    "model.compile(optimizer=optimizer, loss=loss_function)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images data \n",
    "folders = '0123456789abcdef'\n",
    "dataset = 'train'\n",
    "\n",
    "i = folders[0]\n",
    "j = folders[0]\n",
    "k = folders[0]\n",
    "\n",
    "# for i in folders[0:1]:\n",
    "#     for j in folders[0:1]:\n",
    "#         for k in folders[1:2]:\n",
    "\n",
    "print(f\"### {i} - {j} - {k} ###\")\n",
    "\n",
    "path = f'bms-molecular-translation/{dataset}/{i}/{j}/{k}/'\n",
    "\n",
    "for e in range(1):\n",
    "    \n",
    "    print(f\"# Epoch: {e}\")\n",
    "    \n",
    "    score_list = []\n",
    "\n",
    "    # Files\n",
    "    list_names = os.listdir(path)\n",
    "    list_paths = [path for _ in list_names]\n",
    "\n",
    "    # Image data\n",
    "    ImageSet = ImageSetObject(list_names, list_paths)\n",
    "    ImageSet.load_set(new_shape)\n",
    "    data = ImageSet.X\n",
    "\n",
    "    # Text targets\n",
    "    list_id = [x.split('.')[0] for x in ImageSet.list_names]\n",
    "    targets = train_labels.loc[list_id, 'InChI'].values\n",
    "    targets = [[char2idx[x] for x in target] for target in targets]\n",
    "    targets = pad_sequences(targets, padding='post', maxlen=max_len)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(data, targets, epochs=100, batch_size=batch_size, verbose=1)\n",
    "    model.save_weights(f'outputs/{name}.h5')\n",
    "\n",
    "    # Score \n",
    "    y_true=[''.join([idx2char[int(y)] for y in yy]) for yy in targets]\n",
    "    y_predict=get_text_from_predict(model, data, idx2char)\n",
    "    score_list.append(score(y_true, y_predict))\n",
    "    print(f\"\\t> Score: {np.mean(score_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=[''.join([idx2char[int(y)] for y in yy]) for yy in targets]\n",
    "y_predict=get_text_from_predict(model, data, idx2char)\n",
    "score(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images data \n",
    "dataset = 'train'\n",
    "\n",
    "i = folders[5]\n",
    "j = folders[5]\n",
    "k = folders[5]\n",
    "\n",
    "print(f\"### {i} - {j} - {k} ###\")\n",
    "\n",
    "path = f'bms-molecular-translation/{dataset}/{i}/{j}/{k}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "list_names = os.listdir(path)\n",
    "list_paths = [path for _ in list_names]\n",
    "\n",
    "# Image data\n",
    "ImageSet = ImageSetObject(list_names, list_paths)\n",
    "ImageSet.load_set(new_shape)\n",
    "data_validation = ImageSet.X\n",
    "\n",
    "# Text targets\n",
    "list_id = [x.split('.')[0] for x in ImageSet.list_names]\n",
    "targets = train_labels.loc[list_id, 'InChI'].values\n",
    "targets = [[char2idx[x] for x in target] for target in targets]\n",
    "targets = pad_sequences(targets, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_val_true=[''.join([idx2char[int(y)] for y in yy]) for yy in targets_validation]\n",
    "y_val_predict=get_text_from_predict(model, data_validation, idx2char)\n",
    "score(y_val_true, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_true[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create tf model\n",
    "# model = get_model(max_len, vocab)\n",
    "\n",
    "# # load weights\n",
    "# try:\n",
    "#     model.load_weights(f'outputs/{name}.h5')\n",
    "#     print(\"Loaded model from disk\")\n",
    "# except:\n",
    "#     print(\"No initial model\")\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission\n",
    "sample_submission = pd.read_csv(\"bms-molecular-translation/sample_submission.csv\")\n",
    "sample_submission = sample_submission.set_index('image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images data \n",
    "dataset = 'test'\n",
    "\n",
    "i = folders[5]\n",
    "j = folders[5]\n",
    "k = folders[5]\n",
    "\n",
    "print(f\"### {i} - {j} - {k} ###\")\n",
    "\n",
    "path = f'bms-molecular-translation/{dataset}/{i}/{j}/{k}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "list_names = os.listdir(path)\n",
    "list_paths = [path for _ in list_names]\n",
    "\n",
    "# Image data\n",
    "ImageSet = ImageSetObject(list_names, list_paths)\n",
    "ImageSet.load_set(new_shape)\n",
    "data_test = ImageSet.X\n",
    "\n",
    "# Text targets\n",
    "list_id = [x.split('.')[0] for x in ImageSet.list_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_test_predict=get_text_from_predict(model, data_validation, idx2char)\n",
    "y_test_predict=['InChI='+x for x in y_test_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_test_predict = pd.DataFrame([list_id, y_test_predict], index = ['image_id','InChI']).transpose().set_index('image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.loc[df_y_test_predict.index, 'InChI'] = df_y_test_predict['InChI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "sample_submission = sample_submission.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('outputs/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
